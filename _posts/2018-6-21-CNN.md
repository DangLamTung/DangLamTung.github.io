---
layout: post
title: Convolutional neural networks có gì đó sai sai :3
categories: [Computer Vision, deep learning]
published: true
mathjax: true
background: '/images/post2/background.jpg '
---

Nay được nghỉ Mác, ngồi nhà chán nên viết :3 với cũng chưa thấy bài nào nói về CNN có gì sai bằng tiếng Việt, có gì viết sau này làm blog :)))
*Bài được tham khảo (chưa thể nói là dịch từ https://towardsdatascience.com/what-is-wrong-with-convolutional-neural-networks-75c2ba8fbd6f)
# 1.CNN là gì?
   CNN là một loại mạng neuron phổ biến và có hiệu quả rất cao, được ứng dụng rất nhiều trong xử lý ảnh,....

![an image alt text]({{ site.baseurl }}/images/post2/1.jpg "Inception V3")

_Inception v3_
Các thành phần của CNN gồm:
## 1. Lớp Convolutional (Tích chập)
Theo định nghĩa tích chập là 1 phép toán thực hiện đối với 2 hàm số f và g, kết quả cho ra 1 hàm số thứ 3. Nó được ứng dụng trong xác suất, thống kê, thị giác máy tính (computer vision), xử lý ảnh, xử lý tín hiệu, kỹ thuật điện, học máy, và các phương trình vi phân. (Wikipedia)

![an image alt text]({{ site.baseurl }}/images/post2/2.jpg "an image title")

_Tích chập_

Trong CNN tích chập được sử dụng để tách trích một số đặc trưng của một ma trận, ví dụ như hình trên. Một lớp tích chập sẽ tách một tính chất nào đó như mắt, mũi, bla bla...
Có nhiều lớp tích chập để thu lại càng nhiều feature càng tốt :3 

![an image alt text]({{ site.baseurl }}/images/post2/3.jpg "Feature map")

_Các đặc trưng được tách từ một lớp tích chập của Alexnet (cs231n)_
## 2.Tầng pooling
Mục đích chính và duy nhất của tất cả các mạng neural sinh ra là để giảm thiểu số lượng tính toán cần thiết, căn bản là mạng neuron fully connected có thể xấp xỉ tất cả các  hàm số. Vấn đề này được chứng minh trong bài http://neuralnetworksanddeeplearning.com/chap4.html

Thế nhưng thực tế mà nói thì việc tính toán một mạng fully connected để thực hiện các công việc thực tế thì lại khác :)) ta không biết phải train ra sao, train thê nào để fully connected có thể chạy ngon được, chưa kể có nguy cơ overfit :)))
Vì vậy mới có tầng pooling với mục đích giảm số chiều. Như hình là maxpooling 2x2 để giảm số chiều.
 
![an image alt text]({{ site.baseurl }}/images/post2/4.jpg "Max Pooling")

_MaxPooling_
## 3. Sau cùng là một mạng Fully Connected và Softmax :3
 Tổng kết lại là nó như vầy
![an image alt text]({{ site.baseurl }}/images/post2/5.jpg "CNN")

# 2.Vậy sai sai chỗ nào :3
## 1.Traning tốn rất nhiều data, thời gian, tiền bạc
Thuật toán huấn luyện mạng neural cho đến nay là back-propagation, một giải thuật tối ưu hóa theo kiểu đạo hàm :))) hay nói đúng hơn là một bài cực trị kiểu gt2 trên không gian hàng nghìn chiều :)))

![an image alt text]({{ site.baseurl }}/images/post2/6.jpg "Back propagation")

_back prop :)))_

Thực chất công nghệ “cao” nhất của loài người, phát minh vĩ đại nhất lịch sử bla bla như báo hay nói đến thực chất là mấy cái ma trận được tính toán, chỉnh sửa bla bla sao cho nó dự đoán tốt một cái gì đó, maybe xác suất Đức out vòng bảng World Cup :)))))

Nhưng vấn đề là số chiều của bài toán này quá khủng, nên cần lượng data cỡ vài trăm triệu, toàn phép tính ma trận.

![an image alt text]({{ site.baseurl }}/images/post2/7.jpg "Nvidia GTX 1080")

_Mơ ước của tớ :))_


Chính vì vậy train CNN toàn trên GPU, vì GPU có số lõi lớn, có thể thực hiện tính toán ma trận song song :)) *nghe có vẻ giống FPGA* :)) thực chất là facebook có tuyển người làm FPGA cho AI https://www.facebook.com/careers/jobs/a0I1200000KujvKEAR/ mấy nay luận văn BK cũng lắm ông chơi FPGA, không biết làm ngon không chớ làm mlp cho MNIST là hơi còi đó :))) thực chất CNN ra đời từ tích chập rất phổ biến trong điện tử, với Yann LeCun cũng là dân EE, mà chuyện đó nói sau :))

Về vấn đề training mạng thì cũng kiểu đào bitcoin vậy :) mà đào này là đào tri thức :)) mà cũng không phải ngon đâu :) training nhiều khi cả tháng mà ra kết quả như *** ấy chứ :)) nói chung là cũng không phải ngồi mát ăn bát vàng :))

## 2.Nhiều trường hợp fail rất khó đỡ :)

Đặc điểm của CNN là mỗi Convolutional layer chỉ tách một phần nhỏ của hình ảnh, một feature nào đó, và bỏ đi rất nhiều thông tin, sau đó pooling còn làm tình hình tệ hơn khi mà bỏ đi một số tính chất nữa.

Do đó CNN không nhận biết được mối liên hệ về không gian của hình, ví dụ như 2 hình dưới đây cnn sẽ cho là một:

![an image alt text]({{ site.baseurl }}/images/post2/8.jpg "an image title")

Cũng có thể thấy là hướng của hình với CNN rất quan trọng :)) mấy nay làm facenet mà ăn hành vì định hướng cuả cái hình mà không giống data là no hành :)) deep learning là một môn khoa học, nhưng khi đem vào áp dụng thì là engineering, nhiều người học Computer SCIENCE, nhưng nhiều khi ta chỉ áp dụng chứ yếu tố SCIENCE thì cũng chưa chắc quan trọng bằng ra trường nhiều tiền :)))))))))))))))))))))))

Một số ví dụ fail củ chuối của CNN có thể tìm thấy ở đây:

![an image alt text]({{ site.baseurl }}/images/post2/9.jpg "an image title")

_Trích trong paper Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images_

Để sửa sai thì godfather Hinton đã ra Capsule net như là một hướng mới.

{% include youtube.html id="rTawFwUvnLE" %}

:3 tóm lại CNN ngon nhưng còn nhiều chỗ không ổn
 
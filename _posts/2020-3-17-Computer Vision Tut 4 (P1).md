---
layout: post
title: Computer Vison c∆° b·∫£n (P4)
categories: Computer Vision
published: true
mathjax: true
---

B√†i n√†y ch√∫ng ta s·∫Ω b√†n ƒë·∫øn m·ªôt s·ªë thu·∫≠t to√°n n√¢ng cao, m·ªôt s·ªë thu·∫≠t to√°n x·ª≠ l√Ω h√¨nh th√°i ·∫£nh c√°c b·∫°n c√≥ th·ªÉ tham kh·∫£o ·ªü link trong c√°c b√†i tr∆∞·ªõc.

B√†i n√†y s·∫Ω x√©t ƒë·∫øn m·ªôt s·ªë b√†i to√°n c·ª• th·ªÉ trong Computer Vision thay v√¨ c√°c thu·∫≠t to√°n mang t√≠nh c∆° b·∫£n nh∆∞ c√°c b√†i kh√°c

C√°c thu·∫≠t to√°n trong b√†i n√†y n√≥ ·ªü ƒë·ªô ph·ª©c t·∫°p kh√°c bi·ªát so v·ªõi ph·∫ßn tr∆∞·ªõc n√™n n√≥i chung b·∫°n n√†o hi·ªÉu th√¨ xem th√™m, c√≤n kh√¥ng hi·ªÉu th√¨ √°p d·ª•ng nha :>

## 1.Feature detection v·ªõi OpenCV

V√≠ d·ª• nh∆∞ c√°c b·∫°n mu·ªën t√¨m m·ªôt ƒë·ªëi t∆∞·ª£ng trong ·∫£nh, ho·∫∑c l√† mu·ªën t√¨m xem m·ªôt ·∫£nh c√≥ trong m·ªôt t·∫≠p c√°c ·∫£nh kh√¥ng, th√¨ l√∫c n√†y ch√∫ng ta s·ª≠ d·ª•ng thu·∫≠t to√°n feature matching ƒë·ªÉ t√¨m c√°c ƒë·∫∑c ƒëi·ªÉm ƒë·∫∑c bi·ªát c·ªßa m·ªôt ·∫£nh ƒë·ªÉ so s√°nh v·ªõi c√°c ph·∫ßn kh√°c, ho·∫∑c ·∫£nh kh√°c.

V√≠ d·ª• nh∆∞ c√°c b·∫°n ph·∫£i t√¨m m·ªôt ph·∫ßn c·ªßa ·∫£nh trong m·ªôt ·∫£nh l·ªõn nh∆∞ v·∫ßy:

![image.png]({{ site.baseurl }}/images/tut4/1.png)

C√°c thu·∫≠t to√°n trong h·ªç thu·∫≠t to√°n n√†y l√† c√°c thu·∫≠t to√°n:
* SIFT
* ORB
* SURF
* Harris Corner Detection
* ...

C√°c thu·∫≠t to√°n n√†y c√≥ ph·∫ßn l√Ω thuy·∫øt kh√° l√† n·∫∑ng, m√¨nh nghƒ© m√¨nh kh√¥ng ƒë·ªß kh·∫£ nƒÉng cover l·∫°i h·∫øt n√™n ƒë√¢y l√† link tham kh·∫£o, c√°c v√≠ d·ª• c≈©ng ƒë∆∞·ª£c l·∫•y t·ª´ ƒë√¢y: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_table_of_contents_feature2d/py_table_of_contents_feature2d.html

### Thu·∫≠t to√°n Harris Corner Detection

Thu·∫≠t to√°n n√†y d·ª±a tr√™n m·ªôt "c·ª≠a s·ªï tr∆∞·ª£t" (sliding window):
![SegmentLocal]({{ site.baseurl }}/images/tut4/sliding.gif "segment")

T·ª©c c≈©ng g·∫ßn gi·ªëng nh∆∞ ph√©p convolution, ch√∫ng ta th·ª±c hi·ªán qu√©t h·∫øt c√°c ph·∫ßn c·ªßa ·∫£nh ƒë·ªÉ detect t·∫•t c·∫£ c√°c feature c·ªßa ·∫£nh. 

Trong tr∆∞·ªùng h·ª£p thu·∫≠t to√°n Harris th√¨ th·ª© ta c·∫ßn t√¨m ƒë√≥ l√† c·∫°nh c·ªßa m·ªôt ƒë·ªëi t∆∞·ª£ng. ƒê·ªÉ l√†m ƒëi·ªÅu n√†y, h√†m c·ª≠a s·ªï tr∆∞·ª£t c·ªßa ch√∫ng ta c·∫ßn ph·∫£i nh∆∞ sau:

![image.png]({{ site.baseurl }}/images/tut4/10.png)

(ƒê√°ng s·ª£ ƒë√∫ng kh√¥ng :>, m√† ƒë√∫ng l√† ƒë√°ng s·ª£ th·∫≠t ƒë√≥ :v )

Trong ƒë√≥ xu·∫•t ph√°t ·ªü ƒëi·ªÉm (u,v), ch√∫ng ta th·ª±c hi·ªán ph√©p sliding window W(x,y), sau ƒë√≥ t√≠nh "intensity" c·ªßa ·∫£nh t·∫°i c√°c ƒëi·ªÉm khi ta d·ªãch h√¨nh ƒëi m·ªôt ƒëo·∫°n (x,y). H√†m E(u,v) n√†y l√† m·ª©c ƒë·ªô "kh√°c bi·ªát" gi·ªØa c√°c ƒëi·ªÉm ·∫£nh ·ªü (u,v) so v·ªõi c√°c ƒëi·ªÉm ·ªü kho·∫£ng c√°ch gi·ªõi h·∫°n (x,y). T·ª©c ta c·∫ßn t√¨m ƒëi·ªÉm (u,v) sao cho h√†m E(u,v) l√† l·ªõn nh·∫•t, hay n√≥i c√°ch kh√°c ƒëi·ªÉm (u,v) n√†y nhi·ªÅu kh·∫£ nƒÉng l√† "g√≥c" (conner) c·ªßa m·ªôt ƒë·ªëi t∆∞·ª£ng nh·∫•t. L∆∞u √Ω ch·ªó  b√¨nh ph∆∞∆°ng l√† ƒë·ªÉ h√†m kh√¥ng √¢m (m√¨nh nghƒ© c≈©ng c√≥ th·ªÉ s·ª≠ d·ª•ng tr·ªã tuy·ªát ƒë·ªëi)

ƒê·ªÉ th·ª±c hi·ªán ƒëi·ªÅu n√†y, trong paper ng∆∞·ªùi ta s·ª≠ d·ª•ng ph√©p khai tri·ªÉn Taylor ƒë·ªÉ t√≠nh E(u,v) (d√†nh cho c√°c b·∫°n t·ª± h·ªèi h·ªçc GT l√†m g√¨ :> ) (dƒ© nhi√™n l√† Taylor 2 chi·ªÅu):


![image.png]({{ site.baseurl }}/images/tut4/7.png)


V·ªõi:

![image.png]({{ site.baseurl }}/images/tut4/8.png)

L√† th√†nh ph·∫ßn ƒë·∫°o h√†m theo 2 chi·ªÅu c·ªßa ·∫£nh (ƒë·∫°o h√†m ri√™ng), v·ªõi ƒë·∫°o h√†m n√†y ƒë∆∞·ª£c t√≠nh b·∫±ng to√°n t·ª≠ Sobel ƒë√£ n√≥i ƒë·∫øn ·ªü b√†i tr∆∞·ªõc (t·∫°i sao n√≥ t√≠nh ƒë∆∞·ª£c ƒë·∫°o h√†m ? :v). K·∫øt qu·∫£ h√†m n√†y l√† m·ªôt t·ªïng c√°c ma tr·∫≠n (2 * 2) n√™n k·∫øt qu·∫£ cu·ªëi c√πng thu ƒë∆∞·ª£c s·∫Ω l√† m·ªôt ma tr·∫≠n (2 * 2). T·ª´ ƒë√≥ ch√∫ng ta c√≥ th·ªÉ x√©t t√≠nh "c·∫°nh" hay "g√≥c" nh∆∞ sau:

![image.png]({{ site.baseurl }}/images/tut4/9.png)

V√¨ sao c√≥ c√°i n√†y √† :> ·ª±a d·ªü paper ra th√¨ n√≥ l√† c√¥ng th·ª©c ƒëo ƒë·ªô cong c·ªßa h√¨nh kh√¥ng gian ƒë∆∞a v√†o ~~ m·∫•y c√°i n√†y th·ª±c s·ª± m√¨nh ch·ªãu :"> n√™n ta s·∫Ω bi·∫øt l√† n√≥ d·ª±a tr√™n tr·ªã ri√™ng c·ªßa ma tr·∫≠n n√†y, vi·∫øt l·∫°i l√† $$\lambda_1$$ * $$\lambda_2$$ - $$k(\lambda_1 + \lambda_2)^2$$ v√† c√¥ng th·ª©c n√†y ƒë∆∞·ª£c r√∫t g·ªçn t·ª´ c√°c tr∆∞·ªùng h·ª£p c·ªßa tr·ªã ri√™ng nh∆∞ h√¨nh d∆∞·ªõi, c√¥ng th·ª©c $$\lambda_1$$* $$\lambda_2$$/($$\lambda_1$$ + $$\lambda_2$$) c≈©ng x√†i ƒë∆∞·ª£c: 

Link tham kh·∫£o th√™m: [tham kh·∫£o](https://en.wikipedia.org/wiki/Principal_curvature)

![image.png]({{ site.baseurl }}/images/tut4/2.png)

$$\lambda_1$$, $$\lambda_2$$ l√† c√°c tr·ªã ri√™ng, t·ª´ gi√° tr·ªã c·ªßa c√°c tr·ªã ri√™ng n√†y, ta t√≠nh ra h√†m R, v·ªõi k l√† h·ªá s·ªë c·ªßa thu·∫≠t to√°n ƒë·ªÉ x√°c ƒë·ªãnh ra ƒë∆∞·ª£c n√†o l√† c·∫°nh, n√†o l√† g√≥c. C√≤n v√¨ sao n√≥ li√™n h·ªá th√¨ nh·∫Øc l·∫°i ƒëstt ƒë√≥ l√† det = t√≠ch c√°c tr·ªã ri√™ng v√† trace = t·ªïng c√°c tr·ªã ri√™ng. 
* N·∫øu |R| nh·ªè, ph·∫ßn kh√¥ng gian ta t√≠nh l√† "ph·∫≥ng", t·ª©c kh√¥ng c√≥ c·∫°nh g√≥c g√¨ h·∫øt
* R < < 0 th√¨ $$\lambda_1$$ < < $$\lambda_2$$ ho·∫∑c ng∆∞·ª£c l·∫°i, t·ª©c l√† khu v·ª±c n√†y l√† c·∫°nh
* R l·ªõn, t·ª©c $$\lambda_1$$ v√† $$\lambda_2$$ l·ªõn v√† g·∫ßn b·∫±ng nhau, t·ª©c khu v·ª±c n√†y l√† g√≥c

V·∫≠y l√† ƒë·ªß h·∫°i n√£o r·ªìi ha :>, gi·ªù l√† ·ª©ng d·ª•ng n√≥:

OpenCV c√≥ h√†m cv2.cornerHarris():

* img - H√¨nh v√†o, th∆∞·ªùng ƒë∆∞·ª£c convert th√†nh gray v√† c√≥ ki·ªÉu float32.
* blockSize - ƒê·ªô l·ªõn c·ªßa c·ª≠a s·ªï (th∆∞·ªùng h√¨nh vu√¥ng n√™n ƒë·ªÉ l√† 1 s·ªë th√¥i nh√© :v)
* ksize - ƒê·ªô l·ªõn c·ªßa to√°n t·ª≠ Sobel.
* k - H·ªá s·ªë k c·ªßa thu·∫≠t to√°n.


```python
import cv2
import numpy as np
from matplotlib import pyplot as plt
```


```python
filename = 'chessboard.png'
img = cv2.imread(filename)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

gray = np.float32(gray)
dst = cv2.cornerHarris(gray,2,3,0.04) 

plt.subplot(121),plt.imshow(img,cmap = 'gray')
plt.title('Original Image'), plt.xticks([]), plt.yticks([])
plt.subplot(122),plt.imshow(dst,cmap = 'gray')
plt.title('Conner Image'), plt.xticks([]), plt.yticks([])
plt.show()
```


![png]({{ site.baseurl }}/images/tut4/output_10_0.png)



```python
print(dst)
print(dst.shape)
print(dst.min())
print(dst.max())
print(np.argmax(dst))
```

    [[0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     ...
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]]
    (225, 225)
    -169130020.0
    347701220.0
    6273


C√°c b·∫°n ƒë√£ th·∫•y thu·∫≠t to√°n Harris Conner ƒë√£ t√°ch ƒë∆∞·ª£c c·∫°nh v√† g√≥c ra kh·ªèi h√¨nh n√†y, c√°c gi√° tr·ªã c·∫°nh v√† g√≥c l√† c√°c gi√° tr·ªã max

ƒê·ªÉ hi·ªÉn th·ªã c√°c g√≥c kh√¥ng th√¥i th√¨ ta c·∫ßn d√πng m·ªôt thu·∫≠t to√°n bi·∫øn ƒë·ªïi h√¨nh th√°i ·∫£nh l√† l√†m gi√£n (dilate) 


```python
#result is dilated for marking the corners, not important
dst = cv2.dilate(dst,None)
img[dst>0.01*dst.max()]=[0,0,255]

plt.imshow(img,cmap = 'gray')
plt.title('Conner Image'), plt.xticks([]), plt.yticks([])
plt.show()
```


![png]({{ site.baseurl }}/images/tut4/output_13_0.png)


### C√†i ƒë·∫∑t thu·∫≠t to√°n Harris Corner

Ch√∫ng ta s·∫Ω th·ª≠ c√†i thu·∫≠t to√°n n√†y :v, th·ª±c ra th√¨ kh√¥ng c√≥ √≠ch g√¨ l·∫Øm, c∆° m√† m√¨nh th·∫•y m·∫•y tr∆∞·ªùng kh√°c h·ªçc c√≥ c√°i m√† m√¨nh h·ªçc kh√¥ng c√≥, c·∫£ m√¥n th·ªã gi√°c m√°y v·ªõi m√¥n x·ª≠ l√Ω ·∫£nh m√¨nh h·ªçc qua ƒë·ªÅu kh√¥ng c√≥ m√≥n n√†y, n√™n ch·∫Øc c√°c b·∫°n l√†m b√†i n√†y ƒë·ªÉ √≠t ra c≈©ng hi·ªÉu r√µ h∆°n Python
```python
def Harris_Conner(img,window = 2,k = 0.04):
    ####################
    # img: h√¨nh ƒë∆∞a v√†o
    # window: k√≠ch th∆∞·ªõc c·ª≠a s·ªï
    # k: h·ªá s·ªë k
    ####################
    h,w = img.shape
    R = np.zeros((h,w))
    dx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3) #t√≠nh ƒë·∫°o h√†m theo h∆∞·ªõng x, size = 3
    dy = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3) #t√≠nh ƒë·∫°o h√†m theo h∆∞·ªõng y, size = 3
    
    
    d2x = dx ** 2 # t√≠nh th√†nh ph·∫ßn ƒë·∫°o h√†m c·ªßa Taylor
    d2y = dy ** 2 
    dxy = dx * dy # l∆∞u √Ω ph√©p nh√¢n n√†y l√† nh√¢n t·ª´ng s·ªë v·ªõi nhau, nh√¢n ma tr·∫≠n l√† np.dot()
    
    print(dx.shape)
    # ƒê·ªÉ t√≠nh c·ª≠a s·ªï, ta c√≥ th·ªÉ d√πng v√≤ng for, c∆° m√† l∆∞·ªùi l·∫Øm n√™n ta s·∫Ω x√†i h√†m convolute :)), c∆° b·∫£n l√† n√≥ 
    # qu√©t qua h·∫øt c√°c l·ªõp th√¥i 
    
    kernel = np.ones((window,window)) # Qu√©t m·ªôt c·ª≠a s·ªï to√†n s·ªë 1 c≈©ng gi·ªëng b∆∞·ªõc c·ªông to√†n b·ªô c·ªßa thu·∫≠t to√°n 
    
    i2x = cv2.filter2D(d2x, -1, kernel)# H√†m t√≠nh convolution 
    i2y = cv2.filter2D(d2y, -1, kernel)
    ixy = cv2.filter2D(dxy, -1, kernel)
    
    for i in range(h):
        for j in range(w):
            M = np.matrix([[i2x[i,j],ixy[i,j]] , [ixy[i,j],i2y[i,j]] ],dtype=np.float64)
            R[i,j] = np.linalg.det(M) - k * (np.power(np.trace(M), 2))# Thay v√¨ x√†i vector ri√™ng th√¨ x√†i c√¥ng th·ª©c n√†y

    return R
```


```python
filename = 'chessboard.png'
img = cv2.imread(filename)
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

gray = np.float32(gray)
R = Harris_Conner(gray,window = 2,k = 0.04)
print(np.argmax(R))

img[R>0.01*R.max()]=[0,0,255]

plt.imshow(img,cmap = 'gray')
plt.title('Conner Image'), plt.xticks([]), plt.yticks([])
plt.show()
```

    (225, 225)
    6273


C√°c b·∫°n c√≥ th·ªÉ th·∫•y thu·∫≠t to√°n ho·∫°t ƒë·ªông t·ªët :3, thu·∫≠t to√°n n√†y v√† c√°c thu·∫≠t to√°n feature detection kh√°c ƒë∆∞·ª£c s·ª≠ d·ª•ng trong vi·ªác gh√©p ·∫£nh (·∫£nh 360 ƒë·ªô) ho·∫∑c object detection c≈©ng ƒë∆∞·ª£c.

## Thu·∫≠t to√°n SIFT (Scale-Invariant Feature Transform)
Nh∆∞·ª£c ƒëi·ªÉm c·ªßa Harris Conner l√† n√≥ kh√¥ng th·ªÉ scale ƒë∆∞·ª£c, t·ª©c ·∫£nh khi b·ªã scale s·∫Ω kh√¥ng th·ªÉ detect ƒë∆∞·ª£c c·∫°nh, ƒë·ªìng th·ªùi c≈©ng c√≥ nghƒ©a n√≥ ch·ªâ c√≥ th·ªÉ x√°c ƒë·ªãnh c·∫°nh v√† g√≥c ·ªü m·ªôt m·ª©c ƒë·ªô n√†o ƒë√≥, dƒ© nhi√™n xoay ·∫£nh kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn Harris.
![image.png]({{ site.baseurl }}/images/tut4/3.png)


ƒê·ªÉ th·ª±c hi·ªán thu·∫≠t to√°n c√≥ t√≠nh nƒÉng tr√°nh ƒë∆∞·ª£c vi·ªác scale ·∫£nh h∆∞·ªüng th√¨ c√°ch x·ª≠ l√Ω c·ªßa thu·∫≠t to√°n n√†y nh∆∞ sau:
    
1. Scale-space Extrema Detection
B∆∞·ªõc n√†y √°p d·ª•ng m·ªôt ph√©p convolution l√™n ·∫£nh ·ªü c√°c scale kh√°c nhau ƒë·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒë·∫∑c ƒëi·ªÉm (keypoint) c·ªßa ·∫£nh, vi·ªác scale n√†y c≈©ng gi√∫p cho thu·∫≠t to√°n c√≥ t√≠nh "ch·ªãu scale".

Ph∆∞∆°ng ph√°p th·ª±c hi·ªán l√† √°p d·ª•ng 2 b·ªô l·ªçc Gaussian Blur v·ªõi k√¨ v·ªçng (trung b√¨nh c·ªßa ph√¢n ph·ªëi chu·∫©n) kh√°c nhau l√™n m·ªôt ·∫£nh v√† tr·ª´ cho nhau, ƒë√¢y ƒë∆∞·ª£c g·ªçi l√† Differental of Gaussian, l√† c√°ch t√≠nh ƒë∆°n gi·∫£n c·ªßa to√°n t·ª≠ Laplacian cho ph√¢n ph·ªëi Gaussian (LOG) (h·ªçc tron x√°c su·∫•t th·ªëng k√™ + gi·∫£i t√≠ch 2), [tham kh·∫£o](http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html)

![image.png]({{ site.baseurl }}/images/tut4/4.png)

V·ªõi thu·∫≠t to√°n to√°n t·ª≠ LOG c√≥ k√¨ v·ªçng nh·ªè th√¨ s·∫Ω detect c·∫°nh (g√≥c) nh·ªè t·ªët h∆°n v√† v·ªõi LOG c√≥ k√¨ v·ªçng nh·ªè th√¨ detect c·∫°nh (g√≥c) l·ªõn t·ªët h∆°n.

Sau qu√° tr√¨nh n√†y th√¨ ta s·∫Ω c√≥ c√°c gi√° tr·ªã c·ªßa DoG ti·ªÅm nƒÉng ƒë·ªÉ tr·ªü th√†nh "keypoint" cho qu√° tr√¨nh detect ti·∫øp theo, m·ªói qu√° ph·∫ßn t·ª≠ ƒë∆∞·ª£c so s√°nh v·ªõi c√°c ph·∫ßn t·ª≠ xung quanh ƒë·ªÉ t√¨m ƒëi·ªÉm l·ªõn nh·∫•t.

Scale factor cho thu·∫≠t to√°n n√†y l√† k = $$\sqrt(2)$$, k√¨ v·ªçng = 1.6

![image.png]({{ site.baseurl }}/images/tut4/5.png)

2. Keypoint Localization

Thu·∫≠t to√°n th·ª±c hi·ªán ƒë√°nh d·∫•u v·ªã tr√≠ c·ªßa c√° keypoint trong ·∫£nh, ƒë·ªô ch√≠nh x√°c ƒë∆∞·ª£c b·ªï sung nh·ªù s·ª≠ d·ª•ng th√™m khai tri·ªÉn Taylor (ƒë·ªçc th√™m trong b√°o :> )

ƒê·ªÉ l·ªçc c√°c ƒëi·ªÉm c√≥ gi√° tr·ªã nh·ªè h∆°n m·ªôt threshold n√†o ƒë√≥, t√°c gi·∫£ s·ª≠ d·ª•ng m·ªôt threshold l√† constractThreshold ƒë·ªÉ l·ªçc nh·ªØng th√†nh ph·∫ßn keypoint c√≥ gi√° tr·ªã nh·ªè h∆°n m·ª©c n√†o ƒë√≥ (·ªü ƒë√¢y l√† 0.03)

Ph√©p DoG c√≥ ƒëi·ªÉm b·∫•t l·ª£i l√† n√≥ t·∫≠p trung nhi·ªÅu v√†o c·∫°nh h∆°n, n√™n v·ªõi ph∆∞∆°ng ph√°p gi·ªëng thu·∫≠t to√°n Harris, t√°c gi·∫£ c≈©ng t√≠nh ƒë·∫°o h√†m v√† s·ª≠ d·ª•ng h√†m g·∫ßn t∆∞∆°ng t·ª± h√†m ƒë√£ x√©t ·ªü tr√™n ƒë·ªÉ l·ªçc c√°c c·∫°nh ra. Trong b·∫£n c√†i c·ªßa OpenCV, h·∫±ng s·ªë n√†y g·ªçi l√† edgeThreshold, m·∫∑c ƒë·ªãnh = 10



![image.png]({{ site.baseurl }}/images/tut4/11.png)

3. Orientation Assignment

C√°c keypoint s·∫Ω ƒë∆∞·ª£c g√°n cho m·ªôt gi√° tr·ªã g√≥c ƒë·ªô t∆∞∆°ng ƒë·ªëi so v·ªõi c√°c g√≥c kh√°c nh·∫Øm m·ª•c ƒë√≠ch khi·∫øn cho c√°c keypoint n√†y b·∫•t bi·∫øn d√π cho ƒë·ªëi t∆∞·ª£ng matching c√≥ xoay so v·ªõi ban ƒë·∫ßu. (T√≠nh ch·∫•t n√†y ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong lu·∫≠n vƒÉn c·ªßa m·ªôt anh trong CLB :> ).

4. Keypoint Descriptor
B√¢y gi·ªù t·ª´ c√°c th√¥ng tin v·ªÅ v·ªã tr√≠ v√† h∆∞·ªõng c·ªßa c√° keypoint, ch√∫ng ta c√≥ th·ªÉ t·∫°o ra m√¥ t·∫£ v·ªÅ c√°c keypoint, m·ªói keypoint s·∫Ω c√≥ m·ªôt v√πng 16 * 16 pixel bao quanh, ƒë∆∞·ª£c chia th√†nh 16 block nh·ªè 4 * 4, m·ªói block nh·ªè nh∆∞ v·∫≠y th√¨ 8 vector h∆∞·ªõng ƒë∆∞·ª£c t·∫°o ra, do ƒë√≥ t·ªïng c·ªông c√≥ 128 gi√° tr·ªã c·∫ßn l∆∞u, ch√∫ng t·∫°o th√†nh c√°c vector ƒë·ªÉ l∆∞u v√†o m√¥ t·∫£ c·ªßa thu·∫≠t to√°n. T·ª´ c√°c th√¥ng tin n√†y, ta c√≥ th·ªÉ t√¨m ƒë∆∞·ª£c scale c·ªßa ƒë·ªëi t∆∞·ª£ng, ho·∫∑c g√≥c quay c·ªßa ƒë·ªëi t∆∞·ª£ng.

5. Keypoint Matching
Keypoints gi·ªØa 2 h√¨nh s·∫Ω ƒë∆∞·ª£c match b·∫±ng c√°ch t√¨m h√†ng x√≥m g·∫ßn nh·∫•t, tuy nhi√™n m·ªôt s·ªë tr∆∞·ªùng h·ª£p keypoints g·∫ßn nh·∫•t th·ª© 2 l·∫°i qu√° g·∫ßn c√°i th·ª© 1, c√≥ th·ªÉ do nhi·ªÖu ho·∫∑c c√°c v·∫•n ƒë·ªÅ g√¨ ƒë√≥, trong tr∆∞·ªùng h·ª£p ƒë√≥, t·ªâ s·ªë c·ªßa c√°i g·∫ßn nh·∫•t v√† c√°i th·ª© 2 ƒë∆∞·ª£c t√≠nh, n·∫øu > 0.8 th√¨ b·ªè. Trong b√†i b√°o, vi·ªác n√†y gi·∫£m 90% false match, trong khi ch·ªâ b·ªè 5% tr∆∞·ªùng h·ª£p ƒë√∫ng.

### V·∫≠y l√† qu√° ƒë·ªß ha·ªã n√†o :>, v√†o vi·ªác code th√¥i

√Ä m√† thu·∫≠t to√°n n√†y h∆°n ngon n√™n √¥ng t√°c gi·∫£ ƒëƒÉng k√≠ b·∫£n quy·ªÅn, c≈©ng kh√≥ h∆°n n√™n m√¨nh ch∆∞a c√†i tay ƒë∆∞·ª£c, c√°c b·∫°n c·∫ßn c√†i downgrade opencv xu·ªëng 3.3 ƒë·ªÉ d√πng, h√£y ch·∫Øc ch·∫Øn l√† m√¨nh ƒëang ·ªü trong vituralenv khi th·ª±c h√†nh.Ch·∫°y c√¢u l·ªánh b√™n d∆∞·ªõi:


```python
pip install opencv-python==3.4.2.17 opencv-contrib-python==3.4.2.17
```

    Collecting opencv-python==3.4.2.17
      Downloading opencv_python-3.4.2.17-cp36-cp36m-manylinux1_x86_64.whl (25.0 MB)
    [K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0 MB 1.1 MB/s eta 0:00:01
    [?25hCollecting opencv-contrib-python==3.4.2.17
      Downloading opencv_contrib_python-3.4.2.17-cp36-cp36m-manylinux1_x86_64.whl (30.6 MB)
    [K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.6 MB 2.2 MB/s eta 0:00:01
    [?25hRequirement already satisfied: numpy>=1.11.3 in ./opencv/lib/python3.6/site-packages (from opencv-python==3.4.2.17) (1.18.1)
    Installing collected packages: opencv-python, opencv-contrib-python
      Attempting uninstall: opencv-python
        Found existing installation: opencv-python 3.3.0.10
        Uninstalling opencv-python-3.3.0.10:
          Successfully uninstalled opencv-python-3.3.0.10
      Attempting uninstall: opencv-contrib-python
        Found existing installation: opencv-contrib-python 3.3.0.10
        Uninstalling opencv-contrib-python-3.3.0.10:
          Successfully uninstalled opencv-contrib-python-3.3.0.10
    Successfully installed opencv-contrib-python-3.4.2.17 opencv-python-3.4.2.17
    Note: you may need to restart the kernel to use updated packages.



```python

gray= cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
sift = cv2.xfeatures2d.SIFT_create()
kp = sift.detect(gray,None)

cv2.drawKeypoints(gray,kp,img)

plt.imshow(img,cmap = 'gray')
plt.title('Keypoints Image'), plt.xticks([]), plt.yticks([])
plt.show()
```


![png]({{ site.baseurl }}/images/tut4/output_21_0.png)


ƒê·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c c√°c keypoint ch√∫ng ta s·ª≠ d·ª•ng c√°c c√¢u l·ªánh sau:



```python
sift = cv2.xfeatures2d.SIFT_create()
kp, des = sift.detectAndCompute(gray,None)
```


```python
print(len(kp))
print(kp[1].angle) # ƒê·ªçc g√≥c c·ªßa keypoint 1
print(kp[1].pt) # ƒê·ªçc t·ªça ƒë·ªô c·ªßa keypoint 1
#Ph∆∞∆°ng ph√°p n√†y c≈©ng d√πng ƒë∆∞·ª£c v·ªõi Harris v√† c√°c thu·∫≠t to√°n template matching v√¨ c√πng l√† ƒë·ªëi t∆∞·ª£ng keypoint
des = np.array(des)
print(des.shape)

#Tham kh·∫£o: https://docs.opencv.org/3.4/d2/d29/classcv_1_1KeyPoint.html
```

    166
    2.9671630859375
    (141.49732971191406, 37.73331069946289)
    (166, 128)


Sift c√≥ r·∫•t nhi·ªÅu ·ª©ng d·ª•ng, v√† tr∆∞·ªõc th·ªùi k√¨ deep learning n√≥ l√† state-of-art c·ªßa b√†i to√°n object detection, c∆° m√† gi·ªù th√¨ v·ªõi deep learning th√¨ computer vision th∆∞·ªùng ƒë·ª©a n√†o c≈©ng ƒÉn h√†nh th√¥i xD, nh∆∞ng th·ª±c s·ª± vi·ªác h·ªçc computer vision b√¨nh th∆∞·ªùng r·∫•t c√≥ √≠ch, v√¨ kh√¥ng ph·∫£i c√°i g√¨ c≈©ng phang machine learning, deep learning v√†o.

Ngo√†i ra c√≤n c√≥ thu·∫≠t to√°n ORB, SURF trong ch·ªß ƒë·ªÅ feature matching n√†y, c∆° m√† m√¨nh ƒëu·ªëi v·ªõi m·∫•y c√°i l√Ω thuy·∫øt r·ªìi n√™n ch√∫ng ta ch·ªâ coi ·ª©ng d·ª•ng th√¥i.

## Feature matching ƒë·ªÉ t√¨m v·∫≠t th·ªÉ

### Brute Force matching 

B·∫°n n√†o c√≥ qu√° kh·ª© ƒëen t·ªëi hack wifi ch·∫Øc bi·∫øt Brute Force l√† g√¨ :))) n√≥i chung l√† t√¨m v√† so s√°nh v∆°i nhau th√¥i


```python
import numpy as np
import cv2
from matplotlib import pyplot as plt

img1 = cv2.imread('box.png',0)          # queryImage
img2 = cv2.imread('box_in_scene.png',0) # trainImage

# Initiate SIFT detector (nh∆∞ m·ªôt c∆°n gi√≥, code n√†y m√¨nh cop ·ªü trang opencv m√† n√≥ v·∫´n ƒë·ªÉ SIFT)
orb = cv2.ORB_create()

# find the keypoints and descriptors with SIFT
kp1, des1 = orb.detectAndCompute(img1,None) # T√¨m ra 2 c√°i keypoint c·ªßa 2 h√¨nh 
kp2, des2 = orb.detectAndCompute(img2,None)
```


```python
# create BFMatcher object
bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True) # 

# Match descriptors.
matches = bf.match(des1,des2)

# Sort them in the order of their distance.
matches = sorted(matches, key = lambda x:x.distance)

img3 = np.zeros(img1.shape)
# Draw first 10 matches.
img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:10],img3, flags=2) # h√†m n√†y l√† c·ªßa opencv3.4, kh√° chu·ªëi v√¨ img3 c√≥ ·ªü 2 ch·ªó

plt.imshow(img3),plt.show()
```


![png]({{ site.baseurl }}/images/tut4/output_27_0.png)





    (<matplotlib.image.AxesImage at 0x7fd3de4ca940>, None)



ƒêo·∫°n ch∆∞∆°ng tr√¨nh tr√™n ƒë√£ th·ª±c hi·ªán matching c√°c key ƒë√∫ng v·ªõi nhau r·ªìi :3, ngo√†i ra c√≤n c√≥ c√°c c√°ch kh√°c ƒë·ªÉ match, v√≠ d·ª• nh∆∞ FLANN matcher, c√≥ s·ª≠ d·ª•ng th√™m kNN, tuy nhi√™n ƒë√¢y kh√¥ng ph·∫£i tr·ªçng t√¢m c·ªßa m√¨nh c√°c b·∫°n tham kh·∫£o th√™m ·ªü: [tham kh·∫£o](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_matcher/py_matcher.html)

### Detect object b·∫±ng SIFT feature matching

Ta th·ª±c hi·ªán t√¨m 2 feature nh∆∞ ph·∫ßn ORB l√∫c tr∆∞·ªõc v√† FLANN matcher + t·ªâ l·ªá ƒë√£ n√≥i ·ªü link tr√™n


```python
import numpy as np
import cv2
from matplotlib import pyplot as plt

MIN_MATCH_COUNT = 10                    # S·ªë match t·ªëi thi·ªÉu ƒë·ªÉ x√©t 2 v·∫≠t th·ªÉ l√† 1

img1 = cv2.imread('box.png',0)          # queryImage
img2 = cv2.imread('box_in_scene.png',0) # trainImage

# Initiate SIFT detector
sift = cv2.xfeatures2d.SIFT_create()

# find the keypoints and descriptors with SIFT
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)

FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks = 50)

flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(des1,des2,k=2)

# L∆∞u c√°c keypoint match t·ªët v√†o list good
good = []
for m,n in matches:
    if m.distance < 0.7*n.distance:
        good.append(m)
```

Ti·∫øp theo ƒë·ªÉ th·ª±c hi·ªán t√¨m v√† match 2 ƒë·ªëi t∆∞·ª£ng v·ªõi nhau, ta s·ª≠ s·∫Ω ƒë·∫øm 


```python
if len(good)>MIN_MATCH_COUNT:  
    src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2) # m·∫£ng c√°c ƒëi·ªÉm match c·ªßa h√¨nh 1 
    dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2) # m·∫£ng c√°c ƒëi·ªÉm match c·ªßa h√¨nh 2

    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0) # H√†m t√¨m ph√©p bi·∫øn ƒë·ªïi t·ª´ h√¨nh 1 -> h√¨nh 2 d√πng RANSAC
    matchesMask = mask.ravel().tolist()  # Sau khi t√¨m ƒë∆∞·ª£c ma tr·∫≠n bi·∫øn ƒë·ªïi (ph√©p scale v√† ph√©p xoay)

    h,w = img1.shape    
    pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2) # C√°c ƒëi·ªÉm g√≥c c·ªßa h√¨nh g·ªëc ƒë·ªÉ v·∫Ω ƒë∆∞·ªùng bao
    dst = cv2.perspectiveTransform(pts,M)  # Th·ª±c hi·ªán ph√©p bi·∫øn ƒë·ªïi 

    img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA) # V·∫Ω ƒë∆∞·ªùng bao

else:
    print ("Not enough matches are found - %d/%d" % (len(good),MIN_MATCH_COUNT))
    matchesMask = None
```


```python
# draw_params = dict(matchColor = (0,255,0), # draw matches in green color
#                    singlePointColor = None,
#                    matchesMask = matchesMask, # draw only inliers
#                    flags = 2)

img3 = cv2.drawMatches(img1,kp1,img2,kp2,good,None)

plt.imshow(img3, 'gray'),plt.show()
```


![png]({{ site.baseurl }}/images/tut4/output_33_0.png)





    (<matplotlib.image.AxesImage at 0x7fd3de48ff28>, None)



ƒê·ªÉ s·ª≠ d·ª•ng t·ªët feature matching th√¨ ƒë·ªëi t∆∞·ª£ng c·ªßa ch√∫ng ta ph·∫£i mang nhi·ªÅu ƒë·∫∑c ƒëi·ªÉm ƒë·∫∑c tr∆∞ng, ƒë·ªìng th·ªùi kh√¥ng thay ƒë·ªïi (v√≠ d·ª• nh∆∞ ng∆∞·ªùi th√¨ kh√¥ng c·ªüi √°o ra), ƒë·ªìng th·ªùi c≈©ng ph·∫£i c√≥ m·ªôt h√¨nh ƒë·ªß ƒë·∫∑c ƒëi·ªÉm ƒë∆∞·ª£c track tr∆∞·ªõc khi x·ª≠ l√Ω. V√¨ v·∫≠y ch√∫ng ta c·∫ßn ph·∫£i l·∫•y ƒë∆∞·ª£c v·ªã tr√≠ c·ªßa v·∫≠t th·ªÉ. ƒê·ªÉ l√†m ƒëi·ªÅu n√†y OpenCV c√≥ h√†m getROI:

im = cv2.imread('box_in_scene.png',0)
r = cv2.selectROI(im)
imCrop = im[int(r[1]):int(r[1]+r[3]), int(r[0]):int(r[0]+r[2])]

cv2.imshow("Image", imCrop)
k = cv2.waitKey(5) & 0xFF
if k == 27:
    cv2.destroyAllWindows()

![png]({{ site.baseurl }}/images/tut4/output_37_1.png)

ƒê√¢y l√† 2 thu·∫≠t to√°n Feature Detection r·∫•t ph·ªï bi·∫øn, ƒë∆∞·ª£c d√πng nhi·ªÅu tr∆∞·ªõc khi deep learning th·ªëng tr·ªã computer vision.